{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proper-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.color import lab2rgb, rgb2lab\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-penalty",
   "metadata": {},
   "source": [
    "# Unet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "willing-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "        innermost=False, outermost=False\n",
    "    ):\n",
    "        super(UnetBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None:\n",
    "            input_c = nf\n",
    "            \n",
    "        downconv = nn.Conv2d(\n",
    "            input_c, ni, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni * 2, nf, kernel_size=4,\n",
    "                stride=2, padding=1\n",
    "            )\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni, nf, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni * 2, nf, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout:\n",
    "                up += [nn.Dropout(0.5)]\n",
    "            \n",
    "            model = down + [submodule] + up\n",
    "            \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "induced-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super(Unet, self).__init__()\n",
    "        unet_block = UnetBlock(\n",
    "            num_filters * 8, num_filters * 8, innermost=True\n",
    "        )\n",
    "        \n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(\n",
    "                num_filters * 8, num_filters * 8,\n",
    "                submodule=unet_block, dropout=True\n",
    "            )\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(\n",
    "                out_filters // 2, out_filters, submodule=unet_block\n",
    "            )\n",
    "            out_filters //= 2\n",
    "        \n",
    "        self.model = UnetBlock(\n",
    "            output_c, out_filters, input_c=input_c,\n",
    "            submodule=unet_block, outermost=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-actor",
   "metadata": {},
   "source": [
    "# Discriminator block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "known-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        model = [self.get_layers(\n",
    "            input_c, num_filters, norm=False\n",
    "        )]\n",
    "        model += [\n",
    "            self.get_layers(\n",
    "                num_filters * 2 ** i,\n",
    "                num_filters * 2 ** (i + 1),\n",
    "                s = 1 if i == (n_down - 1) else 2\n",
    "            ) for i in range(n_down)\n",
    "        ]\n",
    "        model += [self.get_layers(\n",
    "            num_filters * 2 ** n_down, 1, s=1, norm=False, act=False\n",
    "        )]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True):\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]\n",
    "        if norm:\n",
    "            layers += [nn.BatchNorm2d(nf)]\n",
    "        \n",
    "        if act:\n",
    "            layers += [nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,072\n",
      "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
      "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 256, 32, 32]         524,288\n",
      "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
      "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
      "            Conv2d-9          [-1, 512, 31, 31]       2,097,152\n",
      "      BatchNorm2d-10          [-1, 512, 31, 31]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
      "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
      "================================================================\n",
      "Total params: 2,765,633\n",
      "Trainable params: 2,765,633\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 45.27\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 56.57\n",
      "----------------------------------------------------------------\n",
      "Discriminator output shape: torch.Size([16, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "discriminator = PatchDiscriminator(3)\n",
    "summary(discriminator.to('cuda'), (3, 256, 256))\n",
    "dummy_input = torch.randn(16, 3, 256, 256)\n",
    "out = discriminator(dummy_input.to('cuda'))\n",
    "print('Discriminator output shape: {}'.format(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-cruise",
   "metadata": {},
   "source": [
    "# Gan Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "excessive-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super(GanLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "            \n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        \n",
    "        return labels.expand_as(preds)\n",
    "    \n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-voice",
   "metadata": {},
   "source": [
    "# Model Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chinese-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "        \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-blank",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secondary-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
    "        beta1=0.5, beta2=0.999, lambda_L1=100.\n",
    "    ):\n",
    "        super(MainModel, self).__init__()\n",
    "        \n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        \n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        \n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion = GanLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "        \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "    \n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "    \n",
    "    def backward_D(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "        \n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab)\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "        \n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "        \n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-sleeping",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "architectural-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg  = self.sum / self.count\n",
    "    \n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "    \n",
    "    return {\n",
    "        'loss_D_fake': loss_D_fake,\n",
    "        'loss_D_real': loss_D_real,\n",
    "        'loss_D': loss_D,\n",
    "        'loss_G_GAN': loss_G_GAN,\n",
    "        'loss_G_L1': loss_G_L1,\n",
    "        'loss_G': loss_G\n",
    "    }\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "        \n",
    "def lab_to_rgb(L, ab):\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "        \n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "\n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "        \n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    \n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "        \n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-duncan",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surprised-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exclusive-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "authentic-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob('data/images/Train/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "twenty-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_test_split(image_paths, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "joint-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, image_path, split='train', size=256):\n",
    "        self.image_path = image_path\n",
    "        self.size = size\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_path[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        lab_img = rgb2lab(img).astype(np.float32)\n",
    "        lab_img = T.ToTensor()(lab_img)\n",
    "        L = lab_img[[0], ...] / 50. - 1. # Between -1 and 1\n",
    "        ab = lab_img[[1, 2], ...] / 110. # Between -1 and 1\n",
    "        \n",
    "        return {'L': L, 'ab': ab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excess-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ColorizationDataset(train_images)\n",
    "validation_data = ColorizationDataset(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "functioning-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_sampler = SequentialSampler(validation_data)\n",
    "val_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-hacker",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blocked-place",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized with norm initialization\n",
      "model initialized with norm initialization\n"
     ]
    }
   ],
   "source": [
    "model = MainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "horizontal-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-mexican",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in tqdm(range(epochs)):\n",
    "    loss_meter_dict = create_loss_meters()\n",
    "    i = 0\n",
    "    \n",
    "    for train_data in tqdm(train_dataloader):\n",
    "        model.setup_input(train_data)\n",
    "        model.optimize()\n",
    "        # function updating the log objects\n",
    "        update_losses(model, loss_meter_dict, count=train_data['L'].size(0))\n",
    "        \n",
    "        i += 1\n",
    "        if i % display_every == 0:\n",
    "            print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "            print(f\"Iteration {i}/{len(train_dataloader)}\")\n",
    "            log_results(loss_meter_dict)\n",
    "            visualize(model, train_data, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exciting-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.net_G.state_dict(), 'generator_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dynamic-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model.net_G(train_data['L'].to('cuda')).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
